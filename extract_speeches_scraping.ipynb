{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislatures = [57, 56, 55, 54, 53, 52]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for legislature in legislatures:\n",
    "    df = pd.read_csv(f'data/deputies/deputies_{legislature}.csv')\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_discourse_links(soup):\n",
    "    table_rows = soup.find_all('td')\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    for row in table_rows:\n",
    "        a_tags = row.find_all('a')\n",
    "        if a_tags:\n",
    "            for a_tag in a_tags:\n",
    "                try:\n",
    "                    link = a_tag['href']\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "                if link.startswith('TextoHTML'):\n",
    "                    # remove \"\\n\", \"\\r\", and \"\\t\"\n",
    "                    link = link.replace('\\n', '')\n",
    "                    link = link.replace('\\r', '')\n",
    "                    link = link.replace('\\t', '')\n",
    "                    links.append(link)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deputies_names = set(df[\"nome\"])\n",
    "data = pd.DataFrame(columns=[\"deputy\", \"discourse\", \"discourse_link\", \"date\", \"phase\"])\n",
    "\n",
    "for deputy in deputies_names:\n",
    "    current_page = 1\n",
    "    all_discourse_links = []\n",
    "    deputy_name = deputy.replace(\" \", \"+\")\n",
    "\n",
    "    while True:\n",
    "        url_table = f\"https://www.camara.leg.br/internet/sitaqweb/resultadoPesquisaDiscursos.asp?CurrentPage={current_page}&txOrador={deputy_name}&txPartido=&txUF=&dtInicio=&dtFim=&txTexto=&txSumario=&basePesq=plenario&CampoOrdenacao=dtSessao&PageSize=100&TipoOrdenacao=DESC&btnPesq=Pesquisar\"\n",
    "        response = requests.get(url_table)\n",
    "        \n",
    "        if response.ok:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            page_discourse_links = extract_discourse_links(soup)\n",
    "        else:\n",
    "            print(f\"Failed to get {deputy} - {response} - page {current_page}\")\n",
    "            continue\n",
    "\n",
    "        if not page_discourse_links:\n",
    "            break\n",
    "        \n",
    "        print(f\"Extracted {len(page_discourse_links)} discourse links for {deputy} - page {current_page}\")\n",
    "        all_discourse_links.extend(page_discourse_links)\n",
    "        \n",
    "        current_page += 1\n",
    "        \n",
    "        time.sleep(5)\n",
    "\n",
    "    # Inner tqdm for discourse links\n",
    "    for discourse in tqdm(all_discourse_links, desc=f\"Processing Discourses of {deputy}\", leave=False):\n",
    "        url_discourse = f\"https://www.camara.leg.br/internet/sitaqweb/{discourse}\"\n",
    "        response = requests.get(url_discourse)\n",
    "        \n",
    "        if not response.ok:\n",
    "            print(f\"Failed to get {url_discourse} of deputy {deputy} - {response}\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract discourse text\n",
    "        discourse_text = soup.find_all('p')\n",
    "        discourse_text = \" \".join([p.text for p in discourse_text])\n",
    "        \n",
    "        # Extract date and phase\n",
    "        date, phase = None, None\n",
    "        tds_right = soup.find_all('td', align='right')\n",
    "        for td in tds_right:\n",
    "            if \"Data\" in td.text:\n",
    "                date = td.text.split(\":\")[1].strip()  # Split to get date part\n",
    "            elif \"Fase\" in td.text:\n",
    "                phase = td.text.split(\":\")[1].strip()  # Split to get phase part\n",
    "\n",
    "        # Add to dataframe\n",
    "        data = pd.concat([data, pd.DataFrame([[deputy, discourse_text, url_discourse, date, phase]], \n",
    "                                             columns=[\"deputy\", \"discourse\", \"discourse_link\", \"date\", \"phase\"])])\n",
    "        \n",
    "        if len(data) % 100 == 0:\n",
    "            data.to_csv(\"data/speeches/speeches.csv\", index=False)\n",
    "            \n",
    "        #time.sleep(5)\n",
    "    \n",
    "    print(f\"Finished {deputy_name}\")\n",
    "\n",
    "# Save final result\n",
    "data.to_csv(\"data/speeches/speeches.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
